# semantic (not only)desktop

technical aspects of program interoperability.

## motivation

What if all the applications in your computer were written in some hypothetical high-speed python, every running program was an instance of some common base class, and the document i am now editing could be accessed by hopping into python REPL and typing in something like: `text = window_focus_history[-2].app.document.text`? What if filtering out all lines containing "what if" would be as easy as `Grep(input = text, invert_match = True, pattern = 'what if').run()'?

## providing data	
	

what you can do when writing your app:
### do nothing
Your app possibly takes some command-line arguments. Maybe it has a configuration file in one of the common formats, like INI or YAML. Your app keeps its internal state in variables, unreadable to the user and other programs.
 
### what if we...

A sparql server might be embedded (rdflib) or standalone (a triplestore). In both cases, we can choose to keep updating it continuously, or to only do it when there is a query that needs to be answered. The tradeoffs are fairly obvious:
	* persistence
	* speed
	* memory

A program may also employ different strategies for different aspects or situations. 
For example, a counter in a tight inner loop probably shouldnt be stored in a triplestore.
	

### orm-ed local vars + embedded a sparql server. 
orm-ed in what way?
#### 


3) rdflib add/query, embedded sparql server or eventual update of triplestore 
4) explicit sparql insert/update/select to a triplestore


As you can see, setting up a framework to automatically expose information is the easy part, and there is a bunch of nice options to choose from for different situations.


## consuming data

Consuming data has a few gotchas. 
### federation
Maybe i would prefer to run a whole bunch of triplestores or relational/cyc..->linked data translators on my system, for performance and security reasons...

### performance
a naive way you might try to query a triplestore is by wrapping it in rdflib SPARQLStore, and calling query(), like you would with a small in-memory store. But this has terrible performance due to all the round-tripping. We need to give the server a longer query where possible, and let it do the planning and matching effectively.

writing sparql select queries up-front is tedious. Even if we would simplify this to writing the minimum, ie, just the triples that have to match, it is a distraction. Imo, the ideal is to have a seamless ORM-like thing, that allows you to access the remote data just as if it was local.


### solution/hack:
	segregate self-contained things into their own graphs.

```
got testcases uri "http://localhost/..", which has rdf type "graph pointer", but it does not have property "graph".
||
got testcases uri "http://localhost/..", which has rdf type "Testcases", but it does not have property "items".
```
```
https://rdf.localhost/last_tau_testcases_parsed a https://rdf.localhost/v1/GraphPointer;
https://rdf.localhost/v1/graph xxxx.
```



### smarter approaches:
* https://franz.com/products/allegrocache/ , or allegro CL running on the same machine as the triplestore. Possibly with Allegro Prologâ„¢ on top.
* picolisp persistence
* https://franz.com/agraph/support/documentation/current/magic-properties.html or the similar thing in blazegraph and probably others.
* http://metamodular.com/Common-Lisp/lispos.html
* https://github.com/cmungall/sparqlprog
```
SPARQL provides a declarative way of querying a triplestore. One of its limitations is the lack of ability to compose queries and reuse repeated patterns across multiple queries. Sparqlprog is an extension of SPARQL and a subset of Prolog for relational rule-oriented programming using SPARQL endpoints.
```
```
This package provides a more natural (from a Prolog point of view) interface to SPARQL endpoints. There are two layers. The first, lower layer, defines a DCG for generating SPARQL queries from a structured term. The second provides a translation from representation that looks more or less like a Prolog goal built from rdf/3 goals (with conjunction, disjunction etc) to a term in the term language understood by the SPARQL DCG.

In addition, the library provides a mechanism to register known SPARQL endpoints so that they can be referred to by a short name, or to enable a query to be run against all registered endpoints.
```

* other code generation options, possibly from TT. Lisp OS, mirage unikernel..


